Use Ollama as the LLM-server.

1. Install Ollama
```bash
brew install ollama
```
Check ollama's offical website.
2. Launch ollama
```bash
ollama serve
```
3. Pull model
```bash
ollama pull qwen2.5
```